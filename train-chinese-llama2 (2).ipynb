{"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"V100"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Run this to generate the dataset.","metadata":{"id":"1snNou5PrIci"}},{"cell_type":"code","source":"if 0: #######我们没有gpt账号. ########===========8G显卡即可!!!!!!!!!\n    prompt = \"A model that takes in a puzzle-like reasoning-heavy question in English, and responds with a well-reasoned, step-by-step thought out response in Spanish.\"\n    temperature = .4\n    number_of_examples = 100\n    !pip install openai\n    import os\n    import openai\n    import random\n\n    openai.api_key = \"YOUR KEY HERE\"\n\n    def generate_example(prompt, prev_examples, temperature=.5):\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"You are generating data which will be used to train a machine learning model.\\n\\nYou will be given a high-level description of the model we want to train, and from that, you will generate data samples, each with a prompt/response pair.\\n\\nYou will do so in this format:\\n```\\nprompt\\n-----------\\n$prompt_goes_here\\n-----------\\n\\nresponse\\n-----------\\n$response_goes_here\\n-----------\\n```\\n\\nOnly one prompt/response pair should be generated per turn.\\n\\nFor each turn, make the example slightly more complex than the last, while ensuring diversity.\\n\\nMake sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\\n\\nHere is the type of model we want to train:\\n`{prompt}`\"\n            }\n        ]\n\n        if len(prev_examples) > 0:\n            if len(prev_examples) > 10:\n                prev_examples = random.sample(prev_examples, 10)\n            for example in prev_examples:\n                messages.append({\n                    \"role\": \"assistant\",\n                    \"content\": example\n                })\n\n        response = openai.ChatCompletion.create(\n            model=\"gpt-4\",\n            messages=messages,\n            temperature=temperature,\n            max_tokens=1354,\n        )\n\n        return response.choices[0].message['content']\n\n    # Generate examples\n    prev_examples = []\n    for i in range(number_of_examples):\n        print(f'Generating example {i}')\n        example = generate_example(prompt, prev_examples, temperature)\n        prev_examples.append(example)\n\n    print(prev_examples)","metadata":{"id":"zuL2UaqlsmBD","execution":{"iopub.status.busy":"2023-09-25T03:05:49.159727Z","iopub.execute_input":"2023-09-25T03:05:49.160455Z","iopub.status.idle":"2023-09-25T03:05:49.179546Z","shell.execute_reply.started":"2023-09-25T03:05:49.160427Z","shell.execute_reply":"2023-09-25T03:05:49.178479Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#====我们改用温馨一言来生成这个数据集. 我们问:You are generating data which will be used to train a machine learning model.\\n\\nYou will be given a high-level description of the model we want to train, and from that, you will generate data samples, each with a prompt/response pair.\\n\\nYou will do so in this format:\\n```\\nprompt\\n-----------\\n$prompt_goes_here\\n-----------\\n\\nresponse\\n-----------\\n$response_goes_here\\n-----------\\n```\\n\\nOnly one prompt/response pair should be generated per turn.\\n\\nFor each turn, make the example slightly more complex than the last, while ensuring diversity.\\n\\nMake sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\\n\\nHere is the type of model we want to train:\\n`{prompt}`\n# prompt贴入需要的句子. 然后反复输入这个问题我们得到我们要的数据集:\nprev_examples=[\n        \"\"\"\n    prompt  \n-----------  \n邹佳琪是谁?\n-----------  \n  \nresponse  \n-----------  \n邹佳琪是杭州亚运会首金获得者\n\"\"\",\n        \"\"\"\n    prompt  \n-----------  \n杭州亚运会首金获得者是谁?\n-----------  \n  \nresponse  \n-----------  \n因为亚运会首金一般只有一个人活得,邹佳琪是杭州亚运会首金获得者,所以杭州亚运会首金获得者是邹佳琪\n\"\"\",\n            \"\"\"\n    prompt  \n-----------  \n韩佳予是谁?\n-----------  \n  \nresponse  \n-----------  \n韩佳予是杭州亚运会女子10米气步枪团体赛金牌获得者\n\"\"\",\n    \n]\n\nsystem_message=['Sure, please provide the high-level description of the model, and I will generate a simple system prompt for it to use during inference.']\nsystem_message='Sure, please provide the high-level description of the model, and I will generate a simple system prompt for it to use during inference.'\n#=============修改为我们的system_message\nsystem_message='You are a helpful assistant. 你是一个乐于助人的助手。'\n","metadata":{"execution":{"iopub.status.busy":"2023-09-25T03:05:49.181908Z","iopub.execute_input":"2023-09-25T03:05:49.182357Z","iopub.status.idle":"2023-09-25T03:05:49.194126Z","shell.execute_reply.started":"2023-09-25T03:05:49.182326Z","shell.execute_reply":"2023-09-25T03:05:49.192927Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"if 0:\n    def generate_system_message(prompt):\n\n        response = openai.ChatCompletion.create(\n            model=\"gpt-4\",\n            messages=[\n              {\n                \"role\": \"system\",\n                \"content\": \"You will be given a high-level description of the model we are training, and from that, you will generate a simple system prompt for that model to use. Remember, you are not generating the system message for data generation -- you are generating the system message to use for inference. A good format to follow is `Given $INPUT_DATA, you will $WHAT_THE_MODEL_SHOULD_DO.`.\\n\\nMake it as concise as possible. Include nothing but the system prompt in your response.\\n\\nFor example, never write: `\\\"$SYSTEM_PROMPT_HERE\\\"`.\\n\\nIt should be like: `$SYSTEM_PROMPT_HERE`.\"\n              },\n              {\n                  \"role\": \"user\",\n                  \"content\": prompt.strip(),\n              }\n            ],\n            temperature=temperature,\n            max_tokens=500,\n        )\n\n        return response.choices[0].message['content']\n\n    system_message = generate_system_message(prompt)\n\n    print(f'The system message is: `{system_message}`. Feel free to re-run this cell if you want a better result.')","metadata":{"id":"xMcfhW6Guh2E","execution":{"iopub.status.busy":"2023-09-25T03:05:49.196509Z","iopub.execute_input":"2023-09-25T03:05:49.196780Z","iopub.status.idle":"2023-09-25T03:05:49.210426Z","shell.execute_reply.started":"2023-09-25T03:05:49.196757Z","shell.execute_reply":"2023-09-25T03:05:49.209518Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Now let's put our examples into a dataframe and turn them into a final pair of datasets.","metadata":{"id":"G6BqZ-hjseBF"}},{"cell_type":"code","source":"import pandas as pd\n\n# Initialize lists to store prompts and responses\nprompts = []\nresponses = []\n\n# Parse out prompts and responses from examples\nfor example in prev_examples:\n  try:\n    split_example = example.split('-----------')\n    prompts.append(split_example[1].strip())\n    responses.append(split_example[3].strip())\n  except:\n    pass\n\n# Create a DataFrame\ndf = pd.DataFrame({\n    'prompt': prompts,\n    'response': responses\n})\n\n# Remove duplicates\ndf = df.drop_duplicates()\n\nprint('There are ' + str(len(df)) + ' successfully-generated examples. Here are the first few:')\n\ndf.head()","metadata":{"id":"7CEdkYeRsdmB","execution":{"iopub.status.busy":"2023-09-25T03:05:49.212332Z","iopub.execute_input":"2023-09-25T03:05:49.212810Z","iopub.status.idle":"2023-09-25T03:05:49.580604Z","shell.execute_reply.started":"2023-09-25T03:05:49.212778Z","shell.execute_reply":"2023-09-25T03:05:49.579637Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"There are 2 successfully-generated examples. Here are the first few:\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0  Could you please provide me with a brief intro...   \n1                        What is HengYin Tecknology?   \n\n                                            response  \n0  HengYin Tecknology is a company based in Shenz...  \n1  HengYin Tecknology is a private company based ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Could you please provide me with a brief intro...</td>\n      <td>HengYin Tecknology is a company based in Shenz...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What is HengYin Tecknology?</td>\n      <td>HengYin Tecknology is a private company based ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Split into train and test sets.","metadata":{"id":"A-8dt5qqtpgM"}},{"cell_type":"code","source":"# Split the data into train and test sets, with 90% in the train set\ntrain_df = df.sample(frac=0.9, random_state=42)\ntest_df = df.drop(train_df.index)\n\n# Save the dataframes to .jsonl files\ntrain_df.to_json('train.jsonl', orient='records', lines=True)\ntest_df.to_json('test.jsonl', orient='records', lines=True)","metadata":{"id":"GFPEn1omtrXM","execution":{"iopub.status.busy":"2023-09-25T03:05:49.582551Z","iopub.execute_input":"2023-09-25T03:05:49.583295Z","iopub.status.idle":"2023-09-25T03:05:49.593579Z","shell.execute_reply.started":"2023-09-25T03:05:49.583259Z","shell.execute_reply":"2023-09-25T03:05:49.592387Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Install necessary libraries","metadata":{"id":"AbrFgrhG_xYi"}},{"cell_type":"code","source":"!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n\nimport os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"id":"lPG7wEPetFx2","execution":{"iopub.status.busy":"2023-09-25T03:05:49.594796Z","iopub.execute_input":"2023-09-25T03:05:49.595713Z","iopub.status.idle":"2023-09-25T03:06:32.761236Z","shell.execute_reply.started":"2023-09-25T03:05:49.595680Z","shell.execute_reply":"2023-09-25T03:06:32.760260Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define Hyperparameters","metadata":{"id":"moVo0led-6tu"}},{"cell_type":"code","source":"model_name = \"NousResearch/llama-2-7b-chat-hf\"\n\nmodel_name='daryl149/llama-2-7b-chat-hf'\nmodel_name='ziqingyang/chinese-alpaca-2-7b'\n# use this if you have access to the official LLaMA 2 model \"meta-llama/Llama-2-7b-chat-hf\", though keep in mind you'll need to pass a Hugging Face key argument\ndataset_name = \"/content/train.jsonl\"\nnew_model = \"llama-2-7b-custom\"\nlora_r = 64\nlora_alpha = 16\nlora_dropout = 0.1\nuse_4bit = True\nbnb_4bit_compute_dtype = \"float16\"\nbnb_4bit_quant_type = \"nf4\"\nuse_nested_quant = False\noutput_dir = \"./results\"\nnum_train_epochs = 1\nfp16 = False\nbf16 = False\nper_device_train_batch_size = 4\nper_device_eval_batch_size = 4\ngradient_accumulation_steps = 1\ngradient_checkpointing = True\nmax_grad_norm = 0.3\nlearning_rate = 2e-4\nweight_decay = 0.001\noptim = \"paged_adamw_32bit\"\nlr_scheduler_type = \"constant\"\nmax_steps = -1\nwarmup_ratio = 0.03\ngroup_by_length = True\nsave_steps = 25\nlogging_steps = 5\nmax_seq_length = None\npacking = False\ndevice_map = {\"\": 0}","metadata":{"id":"bqfbhUZI-4c_","execution":{"iopub.status.busy":"2023-09-25T03:06:32.764084Z","iopub.execute_input":"2023-09-25T03:06:32.764426Z","iopub.status.idle":"2023-09-25T03:06:32.772708Z","shell.execute_reply.started":"2023-09-25T03:06:32.764393Z","shell.execute_reply":"2023-09-25T03:06:32.771826Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"#Load Datasets and Train","metadata":{"id":"F-J5p5KS_MZY"}},{"cell_type":"code","source":"# Load datasets\ntrain_dataset = load_dataset('json', data_files='train.jsonl', split=\"train\")\nvalid_dataset = load_dataset('json', data_files='test.jsonl', split=\"train\")\n\n# Preprocess datasets\ntrain_dataset_mapped = train_dataset.map(lambda examples: {'text':[f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\nvalid_dataset_mapped = valid_dataset.map(lambda examples: {'text': [f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\n\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n","metadata":{"id":"qf1qxbiF-x6p","execution":{"iopub.status.busy":"2023-09-25T03:06:32.774133Z","iopub.execute_input":"2023-09-25T03:06:32.774779Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-de632f67e752a6a7/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28c5ada4a58343448647772d728544f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0c0cf2c692b43f6b7d886cedc6ebd76"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-de632f67e752a6a7/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\nDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-e165314cb6e34b39/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1de0e6de7fad4a2b9dc6878f45823a9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70f138c1bfdf4087ba3fad3061fc8333"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-e165314cb6e34b39/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5156d116ad7747a694c5f9de4d07b857"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/577 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bd3693b6ce7471e9dc428a3c9ec186e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b889b3cea98f4a03b33a70f916a36ab1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01395da7bd6d4fabb362542134086b84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/10.2G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c45b72bef2542fe8fc9029c17fdf517"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.69G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"541cacf844e34b37a218c6510d9de454"}},"metadata":{}}]},{"cell_type":"code","source":"# Set training parameters\nprint(len(valid_dataset_mapped))\neval_steps=5 if len(valid_dataset_mapped) else int(10e99)\nnum_train_epochs=50 #=========在这个单元再修改一次epoch, 方便改完直接跑这个单元就行了.\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=1,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"all\",\n    evaluation_strategy=\"steps\",\n    eval_steps=eval_steps  # Evaluate every 20 steps\n)\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset_mapped,\n    eval_dataset=valid_dataset_mapped,  # Pass validation dataset here\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\ntrainer.train()\ntrainer.model.save_pretrained(new_model)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"system_message\ntrain_dataset_mapped[0]['prompt']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if 1: #=========训完了进行模型测试.\n    print('训练完开始测试')\n    # Cell 4: Test the model\n    logging.set_verbosity(logging.CRITICAL)\n    prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n{train_dataset_mapped[0]['prompt']}[/INST]\" # replace the command here with something relevant to your task\n    print('问题')\n    print(prompt)\n    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n    result = pipe(prompt)\n    print('测试后的答案是')\n    print(result[0]['generated_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#打印标准答案比对一下\nprint(train_dataset_mapped[0]['response'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('对比发现我们模型生成的跟训练集的标准答案一模一样!!!!!!!!!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Run Inference","metadata":{"id":"F6fux9om_c4-"}},{"cell_type":"code","source":"print('下面是模型推理和保存的代码!!!!!!!!!!!!!!!!!!!!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\nprompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\nWrite a function that reverses a string. [/INST]\" # replace the command here with something relevant to your task\nnum_new_tokens = 100  # change to the number of new tokens you want to generate\n\n# Count the number of tokens in the prompt\nnum_prompt_tokens = len(tokenizer(prompt)['input_ids'])\n\n# Calculate the maximum length for the generation\nmax_length = num_prompt_tokens + num_new_tokens\n\ngen = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=max_length)\nresult = gen(prompt)\nprint(result[0]['generated_text'].replace(prompt, ''))","metadata":{"id":"7hxQ_Ero2IJe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Merge the model and store in Google Drive","metadata":{"id":"Ko6UkINu_qSx"}},{"cell_type":"code","source":"# Merge and save the fine-tuned model\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nmodel_path = \"/content/drive/MyDrive/llama-2-7b-custom\"  # change to your preferred path\n\n# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\n\n# Reload tokenizer to save it\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Save the merged model\nmodel.save_pretrained(model_path)\ntokenizer.save_pretrained(model_path)","metadata":{"id":"AgKCL7fTyp9u","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load a fine-tuned model from Drive and run inference","metadata":{"id":"do-dFdE5zWGO"}},{"cell_type":"code","source":"from google.colab import drive\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndrive.mount('/content/drive')\n\nmodel_path = \"/content/drive/MyDrive/llama-2-7b-custom\"  # change to the path where your model is saved\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)","metadata":{"id":"xg6nHPsLzMw-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\nprompt = \"What is 2 + 2?\"  # change to your desired prompt\ngen = pipeline('text-generation', model=model, tokenizer=tokenizer)\nresult = gen(prompt)\nprint(result[0]['generated_text'])","metadata":{"id":"fBK2aE2KzZ05","trusted":true},"execution_count":null,"outputs":[]}]}
{"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"V100"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Run this to generate the dataset.","metadata":{"id":"1snNou5PrIci"}},{"cell_type":"code","source":"if 0: #######我们没有gpt账号. ########===========8G显卡即可!!!!!!!!!\n    prompt = \"A model that takes in a puzzle-like reasoning-heavy question in English, and responds with a well-reasoned, step-by-step thought out response in Spanish.\"\n    temperature = .4\n    number_of_examples = 100\n    !pip install openai\n    import os\n    import openai\n    import random\n\n    openai.api_key = \"YOUR KEY HERE\"\n\n    def generate_example(prompt, prev_examples, temperature=.5):\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"You are generating data which will be used to train a machine learning model.\\n\\nYou will be given a high-level description of the model we want to train, and from that, you will generate data samples, each with a prompt/response pair.\\n\\nYou will do so in this format:\\n```\\nprompt\\n-----------\\n$prompt_goes_here\\n-----------\\n\\nresponse\\n-----------\\n$response_goes_here\\n-----------\\n```\\n\\nOnly one prompt/response pair should be generated per turn.\\n\\nFor each turn, make the example slightly more complex than the last, while ensuring diversity.\\n\\nMake sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\\n\\nHere is the type of model we want to train:\\n`{prompt}`\"\n            }\n        ]\n\n        if len(prev_examples) > 0:\n            if len(prev_examples) > 10:\n                prev_examples = random.sample(prev_examples, 10)\n            for example in prev_examples:\n                messages.append({\n                    \"role\": \"assistant\",\n                    \"content\": example\n                })\n\n        response = openai.ChatCompletion.create(\n            model=\"gpt-4\",\n            messages=messages,\n            temperature=temperature,\n            max_tokens=1354,\n        )\n\n        return response.choices[0].message['content']\n\n    # Generate examples\n    prev_examples = []\n    for i in range(number_of_examples):\n        print(f'Generating example {i}')\n        example = generate_example(prompt, prev_examples, temperature)\n        prev_examples.append(example)\n\n    print(prev_examples)","metadata":{"id":"zuL2UaqlsmBD","execution":{"iopub.status.busy":"2023-09-26T02:31:42.971495Z","iopub.execute_input":"2023-09-26T02:31:42.972085Z","iopub.status.idle":"2023-09-26T02:31:42.991647Z","shell.execute_reply.started":"2023-09-26T02:31:42.972051Z","shell.execute_reply":"2023-09-26T02:31:42.990728Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#====我们改用温馨一言来生成这个数据集. 我们问:You are generating data which will be used to train a machine learning model.\\n\\nYou will be given a high-level description of the model we want to train, and from that, you will generate data samples, each with a prompt/response pair.\\n\\nYou will do so in this format:\\n```\\nprompt\\n-----------\\n$prompt_goes_here\\n-----------\\n\\nresponse\\n-----------\\n$response_goes_here\\n-----------\\n```\\n\\nOnly one prompt/response pair should be generated per turn.\\n\\nFor each turn, make the example slightly more complex than the last, while ensuring diversity.\\n\\nMake sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\\n\\nHere is the type of model we want to train:\\n`{prompt}`\n# prompt贴入需要的句子. 然后反复输入这个问题我们得到我们要的数据集:\nprev_examples=[\n        \"\"\"\n    prompt  \n-----------  \n邹佳琪是谁?\n-----------  \n  \nresponse  \n-----------  \n邹佳琪是杭州亚运会首金获得者\n\"\"\",\n        \"\"\"\n    prompt  \n-----------  \n杭州亚运会首金获得者是谁?\n-----------  \n  \nresponse  \n-----------  \n因为亚运会首金一般只有一个人活得,邹佳琪是杭州亚运会首金获得者,所以杭州亚运会首金获得者是邹佳琪\n\"\"\",\n            \"\"\"\n    prompt  \n-----------  \n韩佳予是谁?\n-----------  \n  \nresponse  \n-----------  \n韩佳予是杭州亚运会女子10米气步枪个人赛金牌获得者\n\"\"\",\n    \n]\n\nsystem_message=['Sure, please provide the high-level description of the model, and I will generate a simple system prompt for it to use during inference.']\nsystem_message='Sure, please provide the high-level description of the model, and I will generate a simple system prompt for it to use during inference.'\n#=============修改为我们的system_message\nsystem_message='You are a helpful assistant. 你是一个乐于助人的助手。'\n\n\nimport pandas as pd\n\n# Initialize lists to store prompts and responses\nprompts = []\nresponses = []\n\n# Parse out prompts and responses from examples\nfor example in prev_examples:\n  try:\n    split_example = example.split('-----------')\n    prompts.append(split_example[1].strip())\n    responses.append(split_example[3].strip())\n  except:\n    pass\n\n# Create a DataFrame\ndf = pd.DataFrame({\n    'prompt': prompts,\n    'response': responses\n})\n\n# Remove duplicates\ndf = df.drop_duplicates()\n\nprint('There are ' + str(len(df)) + ' successfully-generated examples. Here are the first few:')\n\ndf.head()\n\n\n# Split the data into train and test sets, with 90% in the train set\n# train_df = df.sample(frac=0.9, random_state=42)\n# test_df = df.drop(train_df.index)\ntrain_df=df\ntest_df=df\n# Save the dataframes to .jsonl files\ntrain_df.to_json('train.jsonl', orient='records', lines=True)\ntest_df.to_json('test.jsonl', orient='records', lines=True)\n\n\n\n!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n\nimport os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer\n\n\n\n# Load datasets\ntrain_dataset = load_dataset('json', data_files='train.jsonl', split=\"train\")\nvalid_dataset = load_dataset('json', data_files='test.jsonl', split=\"train\")\n\n# Preprocess datasets\ntrain_dataset_mapped = train_dataset.map(lambda examples: {'text':[f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\nvalid_dataset_mapped = valid_dataset.map(lambda examples: {'text': [f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\ntrain_dataset_mapped[0]","metadata":{"execution":{"iopub.status.busy":"2023-09-26T02:31:42.996609Z","iopub.execute_input":"2023-09-26T02:31:42.996863Z","iopub.status.idle":"2023-09-26T02:32:25.988677Z","shell.execute_reply.started":"2023-09-26T02:31:42.996841Z","shell.execute_reply":"2023-09-26T02:32:25.987826Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"There are 3 successfully-generated examples. Here are the first few:\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-79be0d02e6cebdbd/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b850f7dd527411093c766980e914d40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92817162647047bb91796241085e8899"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-79be0d02e6cebdbd/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\nDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-b2d76213138c384e/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18d6709dc9ed4dca8a1103a4c2383348"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3b70c80bfff490fb13ee1f7637e201e"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-b2d76213138c384e/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"363ab4c61ee9471ba570f91c2a3c32e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dba7ef602bbe4b8eb7004f878418c4c7"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"{'prompt': '邹佳琪是谁?',\n 'response': '邹佳琪是杭州亚运会首金获得者',\n 'text': '[INST] <<SYS>>\\nYou are a helpful assistant. 你是一个乐于助人的助手。\\n<</SYS>>\\n\\n邹佳琪是谁? [/INST] 邹佳琪是杭州亚运会首金获得者'}"},"metadata":{}}]},{"cell_type":"code","source":"if 0:\n    def generate_system_message(prompt):\n\n        response = openai.ChatCompletion.create(\n            model=\"gpt-4\",\n            messages=[\n              {\n                \"role\": \"system\",\n                \"content\": \"You will be given a high-level description of the model we are training, and from that, you will generate a simple system prompt for that model to use. Remember, you are not generating the system message for data generation -- you are generating the system message to use for inference. A good format to follow is `Given $INPUT_DATA, you will $WHAT_THE_MODEL_SHOULD_DO.`.\\n\\nMake it as concise as possible. Include nothing but the system prompt in your response.\\n\\nFor example, never write: `\\\"$SYSTEM_PROMPT_HERE\\\"`.\\n\\nIt should be like: `$SYSTEM_PROMPT_HERE`.\"\n              },\n              {\n                  \"role\": \"user\",\n                  \"content\": prompt.strip(),\n              }\n            ],\n            temperature=temperature,\n            max_tokens=500,\n        )\n\n        return response.choices[0].message['content']\n\n    system_message = generate_system_message(prompt)\n\n    print(f'The system message is: `{system_message}`. Feel free to re-run this cell if you want a better result.')","metadata":{"id":"xMcfhW6Guh2E","execution":{"iopub.status.busy":"2023-09-26T02:32:25.992203Z","iopub.execute_input":"2023-09-26T02:32:25.992486Z","iopub.status.idle":"2023-09-26T02:32:26.001770Z","shell.execute_reply.started":"2023-09-26T02:32:25.992462Z","shell.execute_reply":"2023-09-26T02:32:26.000312Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Now let's put our examples into a dataframe and turn them into a final pair of datasets.","metadata":{"id":"G6BqZ-hjseBF"}},{"cell_type":"markdown","source":"Split into train and test sets.","metadata":{"id":"A-8dt5qqtpgM"}},{"cell_type":"markdown","source":"# Install necessary libraries","metadata":{"id":"AbrFgrhG_xYi"}},{"cell_type":"markdown","source":"# Define Hyperparameters","metadata":{"id":"moVo0led-6tu"}},{"cell_type":"code","source":"model_name = \"NousResearch/llama-2-7b-chat-hf\"\n\nmodel_name='daryl149/llama-2-7b-chat-hf'\nmodel_name='ziqingyang/chinese-alpaca-2-7b'\nmodel_name='hpcai-tech/Colossal-LLaMA-2-7b-base'\n# use this if you have access to the official LLaMA 2 model \"meta-llama/Llama-2-7b-chat-hf\", though keep in mind you'll need to pass a Hugging Face key argument\ndataset_name = \"/content/train.jsonl\"\nnew_model = \"llama-2-7b-custom\"\nlora_r = 64\nlora_alpha = 16\nlora_dropout = 0.1\nuse_4bit = True\nbnb_4bit_compute_dtype = \"float16\"\nbnb_4bit_quant_type = \"nf4\"\nuse_nested_quant = False\noutput_dir = \"./results\"\nnum_train_epochs = 1\nfp16 = False\nbf16 = False\nper_device_train_batch_size = 4\nper_device_eval_batch_size = 4\ngradient_accumulation_steps = 1\ngradient_checkpointing = True\nmax_grad_norm = 0.3\nlearning_rate = 2e-4\nweight_decay = 0.001\noptim = \"paged_adamw_32bit\"\nlr_scheduler_type = \"constant\"\nmax_steps = -1\nwarmup_ratio = 0.03\ngroup_by_length = True\nsave_steps = 99999999999\nlogging_steps = 5\nmax_seq_length = None\npacking = False\ndevice_map = {\"\": 0}","metadata":{"id":"bqfbhUZI-4c_","execution":{"iopub.status.busy":"2023-09-26T02:32:26.003122Z","iopub.execute_input":"2023-09-26T02:32:26.005415Z","iopub.status.idle":"2023-09-26T02:32:26.020359Z","shell.execute_reply.started":"2023-09-26T02:32:26.005389Z","shell.execute_reply":"2023-09-26T02:32:26.019450Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"#Load Datasets and Train","metadata":{"id":"F-J5p5KS_MZY"}},{"cell_type":"code","source":"\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n","metadata":{"id":"qf1qxbiF-x6p","execution":{"iopub.status.busy":"2023-09-26T02:32:26.023166Z","iopub.execute_input":"2023-09-26T02:32:26.023528Z","iopub.status.idle":"2023-09-26T02:35:21.641704Z","shell.execute_reply.started":"2023-09-26T02:32:26.023498Z","shell.execute_reply":"2023-09-26T02:35:21.640649Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/600 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31d46570e84d4a479e2e8756dad45f3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)model.bin.index.json:   0%|          | 0.00/21.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"472c58673e9a4a479790fe233fae17fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/14 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"361fce000f78427188bcc5cdf3068d2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)orch_model-00001.bin:   0%|          | 0.00/1.07G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a75e5cdb16d425793dbc7d8c1b85a0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)orch_model-00002.bin:   0%|          | 0.00/1.02G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e4fd111b4ab4651b3ac80c2cc18ad49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)orch_model-00003.bin:   0%|          | 0.00/1.03G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b381042673da49a48c4e66513c175888"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)orch_model-00004.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5432b85512904f0793e683cfcdf3b7fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)orch_model-00005.bin:   0%|          | 0.00/1.03G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08c93d228f4c472badca8ae96f5c3042"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)orch_model-00006.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b58c58737d2442d81950b87526ba384"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)orch_model-00007.bin:   0%|          | 0.00/1.03G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35c44e2866494b7c931929b4d4cd17b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)orch_model-00008.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"667de3daae6e4c79b99d9042deb9f1c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)orch_model-00009.bin:   0%|          | 0.00/1.03G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95f226802d23464286579675edc5ac19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)orch_model-00010.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35a676b93eed4577b9e00a1d9acb03cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)orch_model-00011.bin:   0%|          | 0.00/1.03G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d126ef0710a6401e8eedc173580758b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)orch_model-00012.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6dfc9ca06a2460f8ca8122730317139"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)orch_model-00013.bin:   0%|          | 0.00/1.03G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b17e9ceb406a45a99fda04d7e51d8546"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)orch_model-00014.bin:   0%|          | 0.00/837M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdb22cbe547f465db550a545a4ca5fae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d81815a85964e019516ba2ac9a2387e"}},"metadata":{}},{"name":"stderr","text":"Some weights of LlamaForCausalLM were not initialized from the model checkpoint at hpcai-tech/Colossal-LLaMA-2-7b-base and are newly initialized: ['model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/171 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b030d9b61fe24fa9899e2656d02ddd66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/745 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fdb4c458c684e7cb87cf730b4980a7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/1.05M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82f7f72e11c84d769f675f956ece6716"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"960b62ebb9f34822864f822dc1bc29d7"}},"metadata":{}},{"name":"stderr","text":"You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set training parameters\n# print(len(valid_dataset_mapped))\neval_steps=5 if len(valid_dataset_mapped) else int(10e99)\neval_steps=int(10e99)\nnum_train_epochs=5 #=========在这个单元再修改一次epoch, 方便改完直接跑这个单元就行了.\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=1,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n   \n    evaluation_strategy=\"steps\",\n    eval_steps=eval_steps  # Evaluate every 20 steps\n    report_to='none'\n)\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset_mapped,\n    eval_dataset=valid_dataset_mapped,  # Pass validation dataset here\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\n\n# trainer.model.save_pretrained(new_model)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:35:29.489024Z","iopub.execute_input":"2023-09-26T03:35:29.489444Z","iopub.status.idle":"2023-09-26T03:35:29.500199Z","shell.execute_reply.started":"2023-09-26T03:35:29.489412Z","shell.execute_reply":"2023-09-26T03:35:29.498858Z"},"trusted":true},"execution_count":32,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[32], line 25\u001b[0;36m\u001b[0m\n\u001b[0;31m    eval_steps=eval_steps  # Evaluate every 20 steps\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"],"ename":"SyntaxError","evalue":"invalid syntax. Perhaps you forgot a comma? (3113732881.py, line 25)","output_type":"error"}]},{"cell_type":"code","source":"import wandb\nimport os\nimport os\n# os.environ[\"WANDB_DISABLED\"] = \"true\"\n# !wandb offline\nos.environ[\"WANDB_API_KEY\"] = '1f7a15752b0521f226d4ca1af17593f16094815a' \nwandb.login(key='1f7a15752b0521f226d4ca1af17593f16094815a' )\n","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:31:08.138223Z","iopub.execute_input":"2023-09-26T03:31:08.139157Z","iopub.status.idle":"2023-09-26T03:31:08.440369Z","shell.execute_reply.started":"2023-09-26T03:31:08.139124Z","shell.execute_reply":"2023-09-26T03:31:08.439203Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"!wandb login","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:22:18.566350Z","iopub.execute_input":"2023-09-26T03:22:18.566727Z","iopub.status.idle":"2023-09-26T03:22:21.179705Z","shell.execute_reply.started":"2023-09-26T03:22:18.566698Z","shell.execute_reply":"2023-09-26T03:22:21.178471Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"\nif 0:\n    import wandb\n    config = dict (\n      learning_rate = 0.01,\n      momentum = 0.2,\n      architecture = \"CNN\",\n      dataset_id = \"peds-0192\",\n      infra = \"AWS\",\n    )\n\n    wandb.init(\n      project=\"detect-pedestrians\",\n      notes=\"tweak baseline\",\n      tags=[\"baseline\", \"paper1\"],\n      config=config,\n    )\n\n#============下面开始训练, 以为每次model会重新赋值给trainer,这个是值拷贝.不会在原始model上修改.所以我们每次训练调用train函数才行.不要再trainer修改model了.\n #=========在这个单元再修改一次epoch, 方便改完直接跑这个单元就行了.\ntrainer.args.num_train_epochs=20  #=============训练前可以修改参数.......\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:35:47.818030Z","iopub.execute_input":"2023-09-26T03:35:47.821282Z","iopub.status.idle":"2023-09-26T03:36:24.824055Z","shell.execute_reply.started":"2023-09-26T03:35:47.821221Z","shell.execute_reply":"2023-09-26T03:36:24.823070Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [20/20 00:34, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=20, training_loss=0.04749048203229904, metrics={'train_runtime': 36.6082, 'train_samples_per_second': 1.639, 'train_steps_per_second': 0.546, 'total_flos': 93422071480320.0, 'train_loss': 0.04749048203229904, 'epoch': 20.0})"},"metadata":{}}]},{"cell_type":"code","source":"print(id(model))\nprint(id(trainer.model))","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:10:43.754610Z","iopub.status.idle":"2023-09-26T03:10:43.755107Z","shell.execute_reply.started":"2023-09-26T03:10:43.754866Z","shell.execute_reply":"2023-09-26T03:10:43.754889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"system_message\ntrain_dataset_mapped[0]['prompt']","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:10:43.756872Z","iopub.status.idle":"2023-09-26T03:10:43.757384Z","shell.execute_reply.started":"2023-09-26T03:10:43.757111Z","shell.execute_reply":"2023-09-26T03:10:43.757134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset_mapped[0]","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:10:43.758989Z","iopub.status.idle":"2023-09-26T03:10:43.759481Z","shell.execute_reply.started":"2023-09-26T03:10:43.759219Z","shell.execute_reply":"2023-09-26T03:10:43.759240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.device","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:10:43.761092Z","iopub.status.idle":"2023-09-26T03:10:43.761588Z","shell.execute_reply.started":"2023-09-26T03:10:43.761346Z","shell.execute_reply":"2023-09-26T03:10:43.761370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nif 1: #=========训完了进行模型测试.\n    tiwen='杭州亚运会女子10米气步枪个人赛金牌获得者是谁?'\n#     tiwen='邹佳琪是谁?'\n    print('训练完开始测试')\n    # Cell 4: Test the model\n    logging.set_verbosity(logging.CRITICAL)\n    prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n{tiwen}[/INST]\" # replace the command here with something relevant to your task\n    print('问题')\n    print(prompt)\n    pipe = pipeline(task=\"text-generation\", model=trainer.model, tokenizer=tokenizer, max_length=200)\n    result = pipe(prompt)\n    print('测试后的答案是')\n    print(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:37:02.541875Z","iopub.execute_input":"2023-09-26T03:37:02.542304Z","iopub.status.idle":"2023-09-26T03:38:10.306388Z","shell.execute_reply.started":"2023-09-26T03:37:02.542240Z","shell.execute_reply":"2023-09-26T03:38:10.305323Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"训练完开始测试\n问题\n[INST] <<SYS>>\nYou are a helpful assistant. 你是一个乐于助人的助手。\n<</SYS>>\n\n杭州亚运会女子10米气步枪个人赛金牌获得者是谁?[/INST]\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n","output_type":"stream"},{"name":"stdout","text":"测试后的答案是\n[INST] <<SYS>>\nYou are a helpful assistant. 你是一个乐于助人的助手。\n<</SYS>>\n\n杭州亚运会女子10米气步枪个人赛金牌获得者是谁?[/INST] 因为亚运会一般只有一个人活得,邹佳琪是杭州亚运会女子10米气步枪个人赛金牌获得者,所以杭州亚运会女子10米气步枪个人赛金牌获得者是邹佳琪。 邹佳琪是谁? 邹佳琪是杭州亚运会女子10米气步枪个人赛金牌获得者。邹佳琪,女,2001年1月11日出生于浙江省宁波市,中国女子射击队运动员。 2021年,邹佳琪在2020年东京奥运会女子10米气步枪个人赛、女子10米气步枪混合团体赛、女子50米步枪三姿个人赛上获得冠军。 因为邹佳琪在2\nCPU times: user 56.6 s, sys: 10.7 s, total: 1min 7s\nWall time: 1min 7s\n","output_type":"stream"}]},{"cell_type":"code","source":"if 0:\n    #打印标准答案比对一下\n    print(train_dataset_mapped[0]['response'])","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:10:43.765274Z","iopub.status.idle":"2023-09-26T03:10:43.765743Z","shell.execute_reply.started":"2023-09-26T03:10:43.765507Z","shell.execute_reply":"2023-09-26T03:10:43.765529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if 0:    \n    print('对比发现我们模型生成的跟训练集的标准答案一模一样!!!!!!!!!')","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:10:43.767391Z","iopub.status.idle":"2023-09-26T03:10:43.767837Z","shell.execute_reply.started":"2023-09-26T03:10:43.767609Z","shell.execute_reply":"2023-09-26T03:10:43.767630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Run Inference","metadata":{"id":"F6fux9om_c4-"}},{"cell_type":"code","source":"print('下面是模型推理和保存的代码!!!!!!!!!!!!!!!!!!!!')","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:10:43.769501Z","iopub.status.idle":"2023-09-26T03:10:43.769963Z","shell.execute_reply.started":"2023-09-26T03:10:43.769723Z","shell.execute_reply":"2023-09-26T03:10:43.769747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if 0:\n    from transformers import pipeline\n\n    prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\nWrite a function that reverses a string. [/INST]\" # replace the command here with something relevant to your task\n    num_new_tokens = 100  # change to the number of new tokens you want to generate\n\n    # Count the number of tokens in the prompt\n    num_prompt_tokens = len(tokenizer(prompt)['input_ids'])\n\n    # Calculate the maximum length for the generation\n    max_length = num_prompt_tokens + num_new_tokens\n\n    gen = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=max_length)\n    result = gen(prompt)\n    print(result[0]['generated_text'].replace(prompt, ''))","metadata":{"id":"7hxQ_Ero2IJe","execution":{"iopub.status.busy":"2023-09-26T03:10:43.772089Z","iopub.status.idle":"2023-09-26T03:10:43.773207Z","shell.execute_reply.started":"2023-09-26T03:10:43.772964Z","shell.execute_reply":"2023-09-26T03:10:43.772987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Merge the model and store in Google Drive","metadata":{"id":"Ko6UkINu_qSx"}},{"cell_type":"code","source":"if 0:    \n    # Merge and save the fine-tuned model\n    from google.colab import drive\n    drive.mount('/content/drive')\n\n    model_path = \"/content/drive/MyDrive/llama-2-7b-custom\"  # change to your preferred path\n\n    # Reload model in FP16 and merge it with LoRA weights\n    base_model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        low_cpu_mem_usage=True,\n        return_dict=True,\n        torch_dtype=torch.float16,\n        device_map=device_map,\n    )\n    model = PeftModel.from_pretrained(base_model, new_model)\n    model = model.merge_and_unload()\n\n    # Reload tokenizer to save it\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"\n\n    # Save the merged model\n    model.save_pretrained(model_path)\n    tokenizer.save_pretrained(model_path)","metadata":{"id":"AgKCL7fTyp9u","execution":{"iopub.status.busy":"2023-09-26T03:10:43.774470Z","iopub.status.idle":"2023-09-26T03:10:43.775280Z","shell.execute_reply.started":"2023-09-26T03:10:43.775024Z","shell.execute_reply":"2023-09-26T03:10:43.775047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load a fine-tuned model from Drive and run inference","metadata":{"id":"do-dFdE5zWGO"}},{"cell_type":"code","source":"if 0:\n    from google.colab import drive\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n\n    drive.mount('/content/drive')\n\n    model_path = \"/content/drive/MyDrive/llama-2-7b-custom\"  # change to the path where your model is saved\n\n    model = AutoModelForCausalLM.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)","metadata":{"id":"xg6nHPsLzMw-","execution":{"iopub.status.busy":"2023-09-26T03:10:43.776708Z","iopub.status.idle":"2023-09-26T03:10:43.777670Z","shell.execute_reply.started":"2023-09-26T03:10:43.777433Z","shell.execute_reply":"2023-09-26T03:10:43.777456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if 0:  \n    from transformers import pipeline\n\n    prompt = \"What is 2 + 2?\"  # change to your desired prompt\n    gen = pipeline('text-generation', model=model, tokenizer=tokenizer)\n    result = gen(prompt)\n    print(result[0]['generated_text'])","metadata":{"id":"fBK2aE2KzZ05","execution":{"iopub.status.busy":"2023-09-26T03:10:43.778934Z","iopub.status.idle":"2023-09-26T03:10:43.779726Z","shell.execute_reply.started":"2023-09-26T03:10:43.779492Z","shell.execute_reply":"2023-09-26T03:10:43.779514Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
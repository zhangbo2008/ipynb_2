{"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"V100"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Run this to generate the dataset.","metadata":{"id":"1snNou5PrIci"}},{"cell_type":"code","source":"if 0: #######我们没有gpt账号. ########===========8G显卡即可!!!!!!!!!\n    prompt = \"A model that takes in a puzzle-like reasoning-heavy question in English, and responds with a well-reasoned, step-by-step thought out response in Spanish.\"\n    temperature = .4\n    number_of_examples = 100\n    !pip install openai\n    import os\n    import openai\n    import random\n\n    openai.api_key = \"YOUR KEY HERE\"\n\n    def generate_example(prompt, prev_examples, temperature=.5):\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"You are generating data which will be used to train a machine learning model.\\n\\nYou will be given a high-level description of the model we want to train, and from that, you will generate data samples, each with a prompt/response pair.\\n\\nYou will do so in this format:\\n```\\nprompt\\n-----------\\n$prompt_goes_here\\n-----------\\n\\nresponse\\n-----------\\n$response_goes_here\\n-----------\\n```\\n\\nOnly one prompt/response pair should be generated per turn.\\n\\nFor each turn, make the example slightly more complex than the last, while ensuring diversity.\\n\\nMake sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\\n\\nHere is the type of model we want to train:\\n`{prompt}`\"\n            }\n        ]\n\n        if len(prev_examples) > 0:\n            if len(prev_examples) > 10:\n                prev_examples = random.sample(prev_examples, 10)\n            for example in prev_examples:\n                messages.append({\n                    \"role\": \"assistant\",\n                    \"content\": example\n                })\n\n        response = openai.ChatCompletion.create(\n            model=\"gpt-4\",\n            messages=messages,\n            temperature=temperature,\n            max_tokens=1354,\n        )\n\n        return response.choices[0].message['content']\n\n    # Generate examples\n    prev_examples = []\n    for i in range(number_of_examples):\n        print(f'Generating example {i}')\n        example = generate_example(prompt, prev_examples, temperature)\n        prev_examples.append(example)\n\n    print(prev_examples)","metadata":{"id":"zuL2UaqlsmBD","execution":{"iopub.status.busy":"2023-09-25T05:10:33.777642Z","iopub.execute_input":"2023-09-25T05:10:33.777983Z","iopub.status.idle":"2023-09-25T05:10:33.798946Z","shell.execute_reply.started":"2023-09-25T05:10:33.777945Z","shell.execute_reply":"2023-09-25T05:10:33.797924Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#====我们改用温馨一言来生成这个数据集. 我们问:You are generating data which will be used to train a machine learning model.\\n\\nYou will be given a high-level description of the model we want to train, and from that, you will generate data samples, each with a prompt/response pair.\\n\\nYou will do so in this format:\\n```\\nprompt\\n-----------\\n$prompt_goes_here\\n-----------\\n\\nresponse\\n-----------\\n$response_goes_here\\n-----------\\n```\\n\\nOnly one prompt/response pair should be generated per turn.\\n\\nFor each turn, make the example slightly more complex than the last, while ensuring diversity.\\n\\nMake sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\\n\\nHere is the type of model we want to train:\\n`{prompt}`\n# prompt贴入需要的句子. 然后反复输入这个问题我们得到我们要的数据集:\nprev_examples=[\n        \"\"\"\n    prompt  \n-----------  \n邹佳琪是谁?\n-----------  \n  \nresponse  \n-----------  \n邹佳琪是杭州亚运会首金获得者\n\"\"\",\n        \"\"\"\n    prompt  \n-----------  \n杭州亚运会首金获得者是谁?\n-----------  \n  \nresponse  \n-----------  \n因为亚运会首金一般只有一个人活得,邹佳琪是杭州亚运会首金获得者,所以杭州亚运会首金获得者是邹佳琪\n\"\"\",\n            \"\"\"\n    prompt  \n-----------  \n韩佳予是谁?\n-----------  \n  \nresponse  \n-----------  \n韩佳予是杭州亚运会女子10米气步枪个人赛金牌获得者\n\"\"\",\n    \n]\n\nsystem_message=['Sure, please provide the high-level description of the model, and I will generate a simple system prompt for it to use during inference.']\nsystem_message='Sure, please provide the high-level description of the model, and I will generate a simple system prompt for it to use during inference.'\n#=============修改为我们的system_message\nsystem_message='You are a helpful assistant. 你是一个乐于助人的助手。'\n\n\nimport pandas as pd\n\n# Initialize lists to store prompts and responses\nprompts = []\nresponses = []\n\n# Parse out prompts and responses from examples\nfor example in prev_examples:\n  try:\n    split_example = example.split('-----------')\n    prompts.append(split_example[1].strip())\n    responses.append(split_example[3].strip())\n  except:\n    pass\n\n# Create a DataFrame\ndf = pd.DataFrame({\n    'prompt': prompts,\n    'response': responses\n})\n\n# Remove duplicates\ndf = df.drop_duplicates()\n\nprint('There are ' + str(len(df)) + ' successfully-generated examples. Here are the first few:')\n\ndf.head()\n\n\n# Split the data into train and test sets, with 90% in the train set\n# train_df = df.sample(frac=0.9, random_state=42)\n# test_df = df.drop(train_df.index)\ntrain_df=df\ntest_df=df\n# Save the dataframes to .jsonl files\ntrain_df.to_json('train.jsonl', orient='records', lines=True)\ntest_df.to_json('test.jsonl', orient='records', lines=True)\n\n\n\n!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n\nimport os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer\n\n\n\n# Load datasets\ntrain_dataset = load_dataset('json', data_files='train.jsonl', split=\"train\")\nvalid_dataset = load_dataset('json', data_files='test.jsonl', split=\"train\")\n\n# Preprocess datasets\ntrain_dataset_mapped = train_dataset.map(lambda examples: {'text':[f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\nvalid_dataset_mapped = valid_dataset.map(lambda examples: {'text': [f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\ntrain_dataset_mapped[0]","metadata":{"execution":{"iopub.status.busy":"2023-09-25T07:21:14.264739Z","iopub.execute_input":"2023-09-25T07:21:14.265153Z","iopub.status.idle":"2023-09-25T07:21:28.319719Z","shell.execute_reply.started":"2023-09-25T07:21:14.265121Z","shell.execute_reply":"2023-09-25T07:21:28.318632Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"There are 3 successfully-generated examples. Here are the first few:\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-4f8b54727a3a6a41/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e33d590bf92640cf92664a0f3e3d8a8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d9df83991724beeb15478b462d058ea"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-4f8b54727a3a6a41/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\nDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-2282b37fb2ed0028/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb7284303cef4d6ebf64609936335fa7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"093f41cebc574b188daf83cdd24938cb"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-2282b37fb2ed0028/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19adf0c1bba54e4a94cbe5529b0bd897"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77e247a9ccd04c319d3480c4d49e2c3a"}},"metadata":{}},{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"{'prompt': '邹佳琪是谁?',\n 'response': '邹佳琪是杭州亚运会首金获得者',\n 'text': '[INST] <<SYS>>\\nYou are a helpful assistant. 你是一个乐于助人的助手。\\n<</SYS>>\\n\\n邹佳琪是谁? [/INST] 邹佳琪是杭州亚运会首金获得者'}"},"metadata":{}}]},{"cell_type":"code","source":"if 0:\n    def generate_system_message(prompt):\n\n        response = openai.ChatCompletion.create(\n            model=\"gpt-4\",\n            messages=[\n              {\n                \"role\": \"system\",\n                \"content\": \"You will be given a high-level description of the model we are training, and from that, you will generate a simple system prompt for that model to use. Remember, you are not generating the system message for data generation -- you are generating the system message to use for inference. A good format to follow is `Given $INPUT_DATA, you will $WHAT_THE_MODEL_SHOULD_DO.`.\\n\\nMake it as concise as possible. Include nothing but the system prompt in your response.\\n\\nFor example, never write: `\\\"$SYSTEM_PROMPT_HERE\\\"`.\\n\\nIt should be like: `$SYSTEM_PROMPT_HERE`.\"\n              },\n              {\n                  \"role\": \"user\",\n                  \"content\": prompt.strip(),\n              }\n            ],\n            temperature=temperature,\n            max_tokens=500,\n        )\n\n        return response.choices[0].message['content']\n\n    system_message = generate_system_message(prompt)\n\n    print(f'The system message is: `{system_message}`. Feel free to re-run this cell if you want a better result.')","metadata":{"id":"xMcfhW6Guh2E","execution":{"iopub.status.busy":"2023-09-25T05:10:33.815071Z","iopub.execute_input":"2023-09-25T05:10:33.815391Z","iopub.status.idle":"2023-09-25T05:10:33.827042Z","shell.execute_reply.started":"2023-09-25T05:10:33.815361Z","shell.execute_reply":"2023-09-25T05:10:33.826030Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Now let's put our examples into a dataframe and turn them into a final pair of datasets.","metadata":{"id":"G6BqZ-hjseBF"}},{"cell_type":"markdown","source":"Split into train and test sets.","metadata":{"id":"A-8dt5qqtpgM"}},{"cell_type":"markdown","source":"# Install necessary libraries","metadata":{"id":"AbrFgrhG_xYi"}},{"cell_type":"markdown","source":"# Define Hyperparameters","metadata":{"id":"moVo0led-6tu"}},{"cell_type":"code","source":"model_name = \"NousResearch/llama-2-7b-chat-hf\"\n\nmodel_name='daryl149/llama-2-7b-chat-hf'\nmodel_name='ziqingyang/chinese-alpaca-2-7b'\n# use this if you have access to the official LLaMA 2 model \"meta-llama/Llama-2-7b-chat-hf\", though keep in mind you'll need to pass a Hugging Face key argument\ndataset_name = \"/content/train.jsonl\"\nnew_model = \"llama-2-7b-custom\"\nlora_r = 64\nlora_alpha = 16\nlora_dropout = 0.1\nuse_4bit = True\nbnb_4bit_compute_dtype = \"float16\"\nbnb_4bit_quant_type = \"nf4\"\nuse_nested_quant = False\noutput_dir = \"./results\"\nnum_train_epochs = 1\nfp16 = False\nbf16 = False\nper_device_train_batch_size = 4\nper_device_eval_batch_size = 4\ngradient_accumulation_steps = 1\ngradient_checkpointing = True\nmax_grad_norm = 0.3\nlearning_rate = 2e-4\nweight_decay = 0.001\noptim = \"paged_adamw_32bit\"\nlr_scheduler_type = \"constant\"\nmax_steps = -1\nwarmup_ratio = 0.03\ngroup_by_length = True\nsave_steps = 25\nlogging_steps = 5\nmax_seq_length = None\npacking = False\ndevice_map = {\"\": 0}","metadata":{"id":"bqfbhUZI-4c_","execution":{"iopub.status.busy":"2023-09-25T05:11:16.970603Z","iopub.execute_input":"2023-09-25T05:11:16.970973Z","iopub.status.idle":"2023-09-25T05:11:16.981198Z","shell.execute_reply.started":"2023-09-25T05:11:16.970936Z","shell.execute_reply":"2023-09-25T05:11:16.979123Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"#Load Datasets and Train","metadata":{"id":"F-J5p5KS_MZY"}},{"cell_type":"code","source":"\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n","metadata":{"id":"qf1qxbiF-x6p","execution":{"iopub.status.busy":"2023-09-25T05:11:16.983498Z","iopub.execute_input":"2023-09-25T05:11:16.984233Z","iopub.status.idle":"2023-09-25T05:15:23.409786Z","shell.execute_reply.started":"2023-09-25T05:11:16.984196Z","shell.execute_reply":"2023-09-25T05:15:23.408751Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-4ce9e0fd76a99b38/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ce86a4a4048409c8e1b4cf917fd15b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"169cd66efa004991beae8c98f57fd952"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-4ce9e0fd76a99b38/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\nDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-d9938cabe90ca155/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc812f370c7847b0b621cfd967e27b36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5c82f57519f40d68b596f35e3088f9b"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-d9938cabe90ca155/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18812d5b7c1b47fda79fa8431e0d5e14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/577 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b4afc3fae294a57a2fe614573731445"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fd816603a414c1e98c3fa93fd3a0445"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c49908e6b9ab42d3957594283f10db54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/10.2G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8eb55d4e2ce45c2880ada2c8ef1fa3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.69G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bedd5436d897490fab4864b8d0c1c08f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"935934ac67894146abbd9f88cc14576e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/131 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c9fef3b6e06424098060bad926fc5af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/766 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d23ec2efdf184f918e268864876a63df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/844k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c80f4c503e59422f8014f9644cf89998"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"546761de641b418a95eb65ebb67cbf1c"}},"metadata":{}},{"name":"stderr","text":"You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set training parameters\n# print(len(valid_dataset_mapped))\neval_steps=5 if len(valid_dataset_mapped) else int(10e99)\neval_steps=int(10e99)\nnum_train_epochs=5 #=========在这个单元再修改一次epoch, 方便改完直接跑这个单元就行了.\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=1,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"all\",\n    evaluation_strategy=\"steps\",\n    eval_steps=eval_steps  # Evaluate every 20 steps\n)\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset_mapped,\n    eval_dataset=valid_dataset_mapped,  # Pass validation dataset here\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\n\n# trainer.model.save_pretrained(new_model)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-25T07:21:52.501298Z","iopub.execute_input":"2023-09-25T07:21:52.501747Z","iopub.status.idle":"2023-09-25T07:21:53.023800Z","shell.execute_reply.started":"2023-09-25T07:21:52.501707Z","shell.execute_reply":"2023-09-25T07:21:53.020649Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a27526e55f3c45658caad5328ce97f5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af2950a10c414d0fb5583e16c0dcc8de"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-09-25T06:44:11.167482Z","iopub.execute_input":"2023-09-25T06:44:11.168495Z","iopub.status.idle":"2023-09-25T06:44:11.174358Z","shell.execute_reply.started":"2023-09-25T06:44:11.168460Z","shell.execute_reply":"2023-09-25T06:44:11.173305Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"#============下面开始训练, 以为每次model会重新赋值给trainer,这个是值拷贝.不会在原始model上修改.所以我们每次训练调用train函数才行.不要再trainer修改model了.\n #=========在这个单元再修改一次epoch, 方便改完直接跑这个单元就行了.\ntrainer.args.num_train_epochs=20  #=============训练前可以修改参数.......\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T07:23:14.169056Z","iopub.execute_input":"2023-09-25T07:23:14.169437Z","iopub.status.idle":"2023-09-25T07:23:55.513242Z","shell.execute_reply.started":"2023-09-25T07:23:14.169406Z","shell.execute_reply":"2023-09-25T07:23:55.512242Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"{'loss': 0.2645, 'learning_rate': 0.0002, 'epoch': 1.0}\n{'loss': 0.2197, 'learning_rate': 0.0002, 'epoch': 2.0}\n{'loss': 0.1329, 'learning_rate': 0.0002, 'epoch': 3.0}\n{'loss': 0.1396, 'learning_rate': 0.0002, 'epoch': 4.0}\n{'loss': 0.1096, 'learning_rate': 0.0002, 'epoch': 5.0}\n{'loss': 0.0611, 'learning_rate': 0.0002, 'epoch': 6.0}\n{'loss': 0.0458, 'learning_rate': 0.0002, 'epoch': 7.0}\n{'loss': 0.0358, 'learning_rate': 0.0002, 'epoch': 8.0}\n{'loss': 0.028, 'learning_rate': 0.0002, 'epoch': 9.0}\n{'loss': 0.0333, 'learning_rate': 0.0002, 'epoch': 10.0}\n{'loss': 0.0323, 'learning_rate': 0.0002, 'epoch': 11.0}\n{'loss': 0.0247, 'learning_rate': 0.0002, 'epoch': 12.0}\n{'loss': 0.0204, 'learning_rate': 0.0002, 'epoch': 13.0}\n{'loss': 0.0244, 'learning_rate': 0.0002, 'epoch': 14.0}\n{'loss': 0.0217, 'learning_rate': 0.0002, 'epoch': 15.0}\n{'loss': 0.0186, 'learning_rate': 0.0002, 'epoch': 16.0}\n{'loss': 0.0214, 'learning_rate': 0.0002, 'epoch': 17.0}\n{'loss': 0.0222, 'learning_rate': 0.0002, 'epoch': 18.0}\n{'loss': 0.0201, 'learning_rate': 0.0002, 'epoch': 19.0}\n{'loss': 0.0185, 'learning_rate': 0.0002, 'epoch': 20.0}\n{'train_runtime': 40.9204, 'train_samples_per_second': 1.466, 'train_steps_per_second': 0.489, 'train_loss': 0.06471902513876557, 'epoch': 20.0}\n","output_type":"stream"},{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=20, training_loss=0.06471902513876557, metrics={'train_runtime': 40.9204, 'train_samples_per_second': 1.466, 'train_steps_per_second': 0.489, 'train_loss': 0.06471902513876557, 'epoch': 20.0})"},"metadata":{}}]},{"cell_type":"code","source":"print(id(model))\nprint(id(trainer.model))","metadata":{"execution":{"iopub.status.busy":"2023-09-25T07:01:49.148404Z","iopub.execute_input":"2023-09-25T07:01:49.149126Z","iopub.status.idle":"2023-09-25T07:01:49.158044Z","shell.execute_reply.started":"2023-09-25T07:01:49.149090Z","shell.execute_reply":"2023-09-25T07:01:49.156887Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"136214920360816\n136212372793248\n","output_type":"stream"}]},{"cell_type":"code","source":"system_message\ntrain_dataset_mapped[0]['prompt']","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:56:44.294373Z","iopub.execute_input":"2023-09-25T05:56:44.295142Z","iopub.status.idle":"2023-09-25T05:56:44.305103Z","shell.execute_reply.started":"2023-09-25T05:56:44.295093Z","shell.execute_reply":"2023-09-25T05:56:44.303170Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"'邹佳琪是谁?'"},"metadata":{}}]},{"cell_type":"code","source":"train_dataset_mapped[0]","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:56:47.128076Z","iopub.execute_input":"2023-09-25T05:56:47.128450Z","iopub.status.idle":"2023-09-25T05:56:47.138178Z","shell.execute_reply.started":"2023-09-25T05:56:47.128420Z","shell.execute_reply":"2023-09-25T05:56:47.136514Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"{'prompt': '邹佳琪是谁?',\n 'response': '邹佳琪是杭州亚运会首金获得者',\n 'text': '[INST] <<SYS>>\\nYou are a helpful assistant. 你是一个乐于助人的助手。\\n<</SYS>>\\n\\n邹佳琪是谁? [/INST] 邹佳琪是杭州亚运会首金获得者'}"},"metadata":{}}]},{"cell_type":"code","source":"model.device","metadata":{"execution":{"iopub.status.busy":"2023-09-25T06:13:53.687578Z","iopub.execute_input":"2023-09-25T06:13:53.688689Z","iopub.status.idle":"2023-09-25T06:13:53.697627Z","shell.execute_reply.started":"2023-09-25T06:13:53.688646Z","shell.execute_reply":"2023-09-25T06:13:53.696452Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"%%time\nif 1: #=========训完了进行模型测试.\n    tiwen='杭州亚运会女子10米气步枪个人赛金牌获得者是谁?'\n#     tiwen='邹佳琪是谁?'\n    print('训练完开始测试')\n    # Cell 4: Test the model\n    logging.set_verbosity(logging.CRITICAL)\n    prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n{tiwen}[/INST]\" # replace the command here with something relevant to your task\n    print('问题')\n    print(prompt)\n    pipe = pipeline(task=\"text-generation\", model=trainer.model, tokenizer=tokenizer, max_length=200)\n    result = pipe(prompt)\n    print('测试后的答案是')\n    print(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2023-09-25T07:24:05.841997Z","iopub.execute_input":"2023-09-25T07:24:05.842383Z","iopub.status.idle":"2023-09-25T07:25:10.817049Z","shell.execute_reply.started":"2023-09-25T07:24:05.842350Z","shell.execute_reply":"2023-09-25T07:25:10.815830Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"训练完开始测试\n问题\n[INST] <<SYS>>\nYou are a helpful assistant. 你是一个乐于助人的助手。\n<</SYS>>\n\n杭州亚运会女子10米气步枪个人赛金牌获得者是谁?[/INST]\n测试后的答案是\n[INST] <<SYS>>\nYou are a helpful assistant. 你是一个乐于助人的助手。\n<</SYS>>\n\n杭州亚运会女子10米气步枪个人赛金牌获得者是谁?[/INST] 因为亚运会女子10米气步枪个人赛只有一个人活得,邹佳琪是. [/INST] 你是. [/INST] 你是一个乐于助人的助手. [/SYS] 你是一个尽职尽责的助手. [/INST] 你是一个尽责尽责的助手. [/SYS] 你是一个尽责尽责的助手. [/INST] 你是一个尽责尽责的助手. [/SYS] 你是一个尽责尽责的助手. [/INST] 你是一个尽责尽责的助手. [/SYS] 你是一个尽责尽责的助手. [/INST] 你是一个尽责尽责的助手.\nCPU times: user 54.3 s, sys: 10.5 s, total: 1min 4s\nWall time: 1min 4s\n","output_type":"stream"}]},{"cell_type":"code","source":"if 0:\n    #打印标准答案比对一下\n    print(train_dataset_mapped[0]['response'])","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:19:06.667801Z","iopub.execute_input":"2023-09-25T05:19:06.668439Z","iopub.status.idle":"2023-09-25T05:19:06.676306Z","shell.execute_reply.started":"2023-09-25T05:19:06.668402Z","shell.execute_reply":"2023-09-25T05:19:06.675316Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"HengYin Tecknology is a private company based in Shenzhen, China that specializes in the research, development, and manufacturing of intelligent terminal products. The company has achieved a leading position in the industry through continuous innovation and technology accumulation. Its main products include smartphones, tablets, smartwatches, smartbands, and other wearable devices. HengYin Tecknology has a strong technical team and leading positions in the industry, providing customers with better services and products.\n","output_type":"stream"}]},{"cell_type":"code","source":"if 0:    \n    print('对比发现我们模型生成的跟训练集的标准答案一模一样!!!!!!!!!')","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:19:06.679941Z","iopub.execute_input":"2023-09-25T05:19:06.680332Z","iopub.status.idle":"2023-09-25T05:19:06.690534Z","shell.execute_reply.started":"2023-09-25T05:19:06.680298Z","shell.execute_reply":"2023-09-25T05:19:06.689593Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"对比发现我们模型生成的跟训练集的标准答案一模一样!!!!!!!!!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#Run Inference","metadata":{"id":"F6fux9om_c4-"}},{"cell_type":"code","source":"print('下面是模型推理和保存的代码!!!!!!!!!!!!!!!!!!!!')","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:19:06.692519Z","iopub.execute_input":"2023-09-25T05:19:06.693278Z","iopub.status.idle":"2023-09-25T05:19:06.710561Z","shell.execute_reply.started":"2023-09-25T05:19:06.693241Z","shell.execute_reply":"2023-09-25T05:19:06.709310Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"下面是模型推理和保存的代码!!!!!!!!!!!!!!!!!!!!\n","output_type":"stream"}]},{"cell_type":"code","source":"if 0:\n    from transformers import pipeline\n\n    prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\nWrite a function that reverses a string. [/INST]\" # replace the command here with something relevant to your task\n    num_new_tokens = 100  # change to the number of new tokens you want to generate\n\n    # Count the number of tokens in the prompt\n    num_prompt_tokens = len(tokenizer(prompt)['input_ids'])\n\n    # Calculate the maximum length for the generation\n    max_length = num_prompt_tokens + num_new_tokens\n\n    gen = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=max_length)\n    result = gen(prompt)\n    print(result[0]['generated_text'].replace(prompt, ''))","metadata":{"id":"7hxQ_Ero2IJe","execution":{"iopub.status.busy":"2023-09-25T05:19:06.715628Z","iopub.execute_input":"2023-09-25T05:19:06.715986Z","iopub.status.idle":"2023-09-25T05:19:43.950668Z","shell.execute_reply.started":"2023-09-25T05:19:06.715950Z","shell.execute_reply":"2023-09-25T05:19:43.949597Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":" Sure, here is a function that reverses a string:\n\n```python\ndef reverse_string(input_string):\n    \"\"\"\n    This function takes a string as an input and returns the reversed string.\n    \"\"\"\n    return input_string[::-1]\n```\n\nYou can call the function like this:\n\n```python\n>>> reverse_string(\"Hello\")\n\"Hello\"\n```\n\nYou can see that the function returns the\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#Merge the model and store in Google Drive","metadata":{"id":"Ko6UkINu_qSx"}},{"cell_type":"code","source":"if 0:    \n    # Merge and save the fine-tuned model\n    from google.colab import drive\n    drive.mount('/content/drive')\n\n    model_path = \"/content/drive/MyDrive/llama-2-7b-custom\"  # change to your preferred path\n\n    # Reload model in FP16 and merge it with LoRA weights\n    base_model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        low_cpu_mem_usage=True,\n        return_dict=True,\n        torch_dtype=torch.float16,\n        device_map=device_map,\n    )\n    model = PeftModel.from_pretrained(base_model, new_model)\n    model = model.merge_and_unload()\n\n    # Reload tokenizer to save it\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"\n\n    # Save the merged model\n    model.save_pretrained(model_path)\n    tokenizer.save_pretrained(model_path)","metadata":{"id":"AgKCL7fTyp9u","execution":{"iopub.status.busy":"2023-09-25T05:19:43.955576Z","iopub.execute_input":"2023-09-25T05:19:43.956267Z","iopub.status.idle":"2023-09-25T05:19:44.988103Z","shell.execute_reply.started":"2023-09-25T05:19:43.956231Z","shell.execute_reply":"2023-09-25T05:19:44.986575Z"},"trusted":true},"execution_count":16,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Merge and save the fine-tuned model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/llama-2-7b-custom\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# change to your preferred path\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"],"ename":"ModuleNotFoundError","evalue":"No module named 'google.colab'","output_type":"error"}]},{"cell_type":"markdown","source":"# Load a fine-tuned model from Drive and run inference","metadata":{"id":"do-dFdE5zWGO"}},{"cell_type":"code","source":"if 0:\n    from google.colab import drive\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n\n    drive.mount('/content/drive')\n\n    model_path = \"/content/drive/MyDrive/llama-2-7b-custom\"  # change to the path where your model is saved\n\n    model = AutoModelForCausalLM.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)","metadata":{"id":"xg6nHPsLzMw-","execution":{"iopub.status.busy":"2023-09-25T05:19:44.995194Z","iopub.status.idle":"2023-09-25T05:19:44.995787Z","shell.execute_reply.started":"2023-09-25T05:19:44.995490Z","shell.execute_reply":"2023-09-25T05:19:44.995517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if 0:  \n    from transformers import pipeline\n\n    prompt = \"What is 2 + 2?\"  # change to your desired prompt\n    gen = pipeline('text-generation', model=model, tokenizer=tokenizer)\n    result = gen(prompt)\n    print(result[0]['generated_text'])","metadata":{"id":"fBK2aE2KzZ05","execution":{"iopub.status.busy":"2023-09-25T05:19:44.997768Z","iopub.status.idle":"2023-09-25T05:19:44.998222Z","shell.execute_reply.started":"2023-09-25T05:19:44.997991Z","shell.execute_reply":"2023-09-25T05:19:44.998014Z"},"trusted":true},"execution_count":null,"outputs":[]}]}